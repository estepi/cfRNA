---
title: "Nexflow_pipelines"
author: "Estefania Mancini"
date: "2022-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Nexflow 

```{bash rnaseqDL_full, eval=FALSE}
aws batch submit-job \
    --job-name rnaseqDL_full\
    --job-queue default-multiplai-nextflow\
    --job-definition nextflow-multiplai-nextflow\
    --container-overrides command=s3://data-team-experimentation/nf-core/rnaseq,\
"--outdir","'s3://data-team-experimentation/nextflow-test_full'",\
"-profile","test_full",\
"--email","'soyestepi@gmail.com'",\
"--multiqc_title","test_full_RNASeq" 

* s3://data-team-experimentation/nextflow-test_full
```



FAILED:

https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Faws$252Fbatch$252Fjob/log-events/nextflow-multiplai-nextflow$252Fdefault$252Ff4767e89fbfa4a7c808fc54a190f3bff

Error:

```{bash wt1, eval=FALSE}
 EXITING because of FATAL PARAMETER ERROR: limitGenomeGenerateRAM=6342450944is too small for your genome
  SOLUTION: please specify --limitGenomeGenerateRAM not less than 8966723168 and make that much RAM available 
```

Solution:

```{bash mem, eval=FALSE}
// Add for STAR
process {
    withName: 'STAR_GENOMEGENERATE' {
        memory = 64.GB
    }
}
```

Error:


```{bash 137, eval=FALSE}

Error executing process > 'NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:SALMON_INDEX (gencode.v39.transcripts.fixed.fa)'

Caused by:
  Essential container in task exited - OutOfMemoryError: Container killed due to memory usage

Command exit status:   137
```

Solution:
See docs, https://nf-co.re/rnaseq/usage#alignment-options

For example, if the nf-core/rnaseq pipeline is failing after multiple re-submissions of the STAR_ALIGN process due to an exit code of 137 this would indicate that there is an out of memory issue:

```{bash memo, eval=FALSE}
// Add for STAR
process {
    withName: 'STAR_GENOMEGENERATE' {
        memory = 64.GB
    }
}
```

Or Add parameter:

```{bash Max_mem, eval=FALSE}
--max_memory 120GB
```

* We can also add INDEXes as parametes:

save indexes once they are built:

```{bash saveRef, eval=FALSE}
 --save_reference
``` 

* Working dir

```{bash wd, eval=FALSE}
  s3://multiplai-nextflow-results/_nextflow/runs/89/d6e38f8d1668aace84a21cba0de9f3
```

Then, we can add:

```{bash wd2, eval=FALSE}
"--input","'s3://data-team-experimentation/WT/wt1.csv'",\
"--outdir","'s3://data-team-experimentation/W1R'",\
"-profile","test",\
"--max_memory","100GB",\
"--gencode",\
"--email","'soyestepi@gmail.com'",\
"--multiqc_title","W1",\
"--star_index","'s3://data-team-experimentation/references/genome/index/star/'",\
"--salmon_index","'s3://data-team-experimentation/references/genome/index/salmon'",\
"--fasta","'s3://data-team-experimentation/references/GRCh38.p13.genome.fa'",\
"--gtf","'s3://data-team-experimentation/references/gencode.v39.chr_patch_hapl_scaff.annotation.gtf'",\
"--transcript_fasta","'s3://data-team-experimentation/references/gencode.v39.transcripts.fa'"
```

*  previamente salvamos los indexes:





```{bash wd3, eval=FALSE}
Error executing process > 'NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:STAR_GENOMEGENERATE (GRCh38.p13.genome.fa)'
Caused by:
  Job attempt duration exceeded timeout
```

* Rm work directory, change memory size

*     Rm test folders (~500Gb)

* Edit STAR:

\https://nfcore.slack.com/archives/CE8SSJV3N/p1666968742980909

* Error SALMON

```{bash salmon, eval=FALSE}
 salmon quant \
      --geneMap gencode.v39.chr_patch_hapl_scaff.annotation.gtf \
      --threads 2 \
      --libType=IU \
      -t gencode.v39.transcripts.fixed.fa \
      -a WT-1.Aligned.toTranscriptome.out.bam \
      -o WT-1
  Command error:
  [2022-11-11 11:11:37.973] [jointLog] [critical] Transcript ENST00000383224.6 appeared in the BAM header, but was not in the provided FASTA file
  [2022-11-11 11:11:37.973] [jointLog] [critical] Please provide a reference FASTA file that includes all targets present in the BAM header
  If you have access to the genome FASTA and GTF used for alignment 
  consider generating a transcriptome fasta using a command like: 
  gffread -w output.fa -g genome.fa genome.gtf
  you can find the gffread utility at (http://ccb.jhu.edu/software/stringtie/gff.shtml)
```    
 
- W1RS
- W1R

* tuve que agregarme de nuevo a las inbound rules

* Salmon index ALan copiar a:

```{bashxps3, eval=FALSE}
aws s3 cp salmon_indexes  s3://data-team-experimentation/references/ --recursive  --include "*"
```


